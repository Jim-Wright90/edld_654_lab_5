---
title: "EDLD 654 Lab 5: Claire, Thuy, Jim"
output:
  html_document: 
    toc: true
    toc_float: true
    theme: "journal"
    css: "website-custom.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(tidymodels)
library(tune)
library(glmnet)
library(baguette)
library(parsnip)
library(doParallel)
library(vip)
library(pdp)
library(patchwork)
library(ranger)
library(future)
```

# Data 

```{r, include=TRUE}
set.seed(3000)
data <- read_csv(here::here("data", "train.csv")) %>% 
  select(-classification)

data <- dplyr::sample_frac(data, size = 0.01)

sheets <- readxl::excel_sheets(here::here("data",
"fallmembershipreport_20192020.xlsx"))

ode_schools <- readxl::read_xlsx(here::here("data",
"fallmembershipreport_20192020.xlsx"), sheet = sheets[4])

ethnicities <- ode_schools %>%
select(attnd_schl_inst_id = `Attending School ID`,
sch_name = `School Name`,
contains("%")) %>%
janitor::clean_names()
names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))

data <- left_join(data, ethnicities)
head(data)
colnames(data)
data %>% 
  select(sch_name)
```

# Split and Resample 

```{r, include=TRUE}
set.seed(3000)
data_split <- initial_split(data, strata = "score")

set.seed(3000)
train <- training(data_split)
test <- testing(data_split)

set.seed(3000)
data_cv <- vfold_cv(train, strata = "score")

```

# Preprocess 

```{r, include=TRUE}
rec <- recipe(
    formula = score ~ ., data = train 
  ) %>%
 step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %>%
 update_role(contains("id"), ncessch, sch_name, new_role = "id") %>%
 step_novel(all_nominal(), -all_outcomes()) %>%
 step_unknown(all_nominal(), -all_outcomes()) %>%
 step_medianimpute(all_numeric()) %>%
 step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>%
 step_dummy(all_nominal(), -has_role(match = "id"), -all_outcomes()) %>%
 step_nzv(all_predictors())



prep(rec)  # do we need save this as an object
```

# Decision Tree 

## Model and Workflow

```{r, include=TRUE}
# 1. Create a parsnip CART model using{rpart} for the estimation, tuning the cost complexity and minimum n for a terminal node.
tune_model <- decision_tree() %>% 
  set_mode("regression") %>% 
  set_engine("rpart") %>% 
  set_args(cost_complexity = tune(), #slide 48 W7p1
           min_n = tune())

# 2. Create a workflow object that combines your recipe and your parsnip objects.

decisiontree_wflow <- workflow() %>%
add_recipe(rec) %>%
add_model(tune_model)

# Can we take this out now b/c here we're asked to creat workflow object only ?
# is this base model that we are going compare with?
#fit1 <- fit_resamples(decisiontree_wflow, data_cv)
# collect_metrics(fit1)

# 3. Tune your model with tune_grid

grd <- grid_regular(cost_complexity(), min_n(), levels = c(10, 5)) # is that meant by "Use grid = 10 to choose 10 grid points automatically"? Isn't this cost_ = 10 and min = 5?
# where to put grid = 10 to choose 10 grid points automatically?? 
grd

metrics_eval <- metric_set(rmse,
                           rsq,
                           huber_loss)

tictoc::tic()
tune_tree <- tune_grid(tune_model, 
                       rec, 
                       data_cv, 
                       grid = grd, 
                       metrics = metrics_eval)
tictoc::toc()

# the time it takes to run: 353.078 sec elapsed

```

## Best Estimates 

```{r, include=TRUE}
# Show the best estimates for each of the three performance metrics and the tuning parameter values associated with each.

show_best(tune_tree, "rmse")
select_best(tune_tree,"rmse")

show_best(tune_tree, "rsq")
select_best(tune_tree,"rsq")

show_best(tune_tree, "huber_loss")
select_best(tune_tree,"huber_loss")

#note: cross three metrics, model48 is the best, both values of cost_complexity and min_n remain the same
```

# Bagged Tree

## Model and Workflow

```{r, include=TRUE}
# 1. Create a parsnip bagged tree model using{baguette}
bag_model_tune <- bag_tree() %>% 
  set_mode("regression") %>% 
  set_args(cost_complexity = tune(), min_n = tune()) %>% 
  set_engine("rpart", times = 10) 

#2. Create a workflow object that combines your recipe and your bagged tree model specification.
bagged_wflow <- workflow() %>%
add_recipe(rec) %>%
add_model(bag_model_tune)

# Do we need this here? 
# bagged_fit <- fit_resamples(bagged_wflow, data_cv)
# collect_metrics(bagged_fit)

# 3. Tune your model with tune_grid

tree_grid <- grid_max_entropy(cost_complexity(), min_n(), size = 10) # Is this "Use grid = 10 to choose 10 grid points automatically" ?

plan(multisession) 
tictoc::tic()
bag_tune <- tune_grid(bag_model_tune,
                      rec,
                      data_cv,
                      grid = tree_grid,
                      metrics = metrics_eval, 
                      control = control_resamples(verbose = TRUE,
                                                  save_pred = TRUE,
                                                  extract = function(x) extract_model(x)))
tictoc::toc()

# time elapsed 164.882 seconds 
# finalize model


```

## Best Estimates 

Answer to question prior to running code:

```{r, include=TRUE}
show_best(bag_tune, "rmse")
select_best(bag_tune, "rmse")

show_best(bag_tune, "rsq")
select_best(bag_tune, "rsq")

show_best(bag_tune, "huber_loss")
select_best(bag_tune, "huber_loss")
```

## Bag Roots Function

```{r, include=TRUE}
bag_roots <- function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(models = map(.extracts,
                  ~.x$model_df)) %>% 
  select(-.extracts) %>% 
  unnest(cols = c(models)) %>% 
  mutate(root = map_chr(model,
                     ~as.character(.x$fit$frame[1, 1]))) %>%
  select(root)  
}

# output the feature at the root node for each of the decision trees fit.
bag_roots(bag_tune)

root <- bag_roots(bag_tune)

```

## Plot

```{r, include=TRUE}
#Produce a plot of the frequency of features at the root node of the trees in your bagged model. --> slide w8p1

root_count <- root %>% 
  count(root)

ggplot(root_count, aes(fct_reorder(root, n), n)) +
  geom_col() +
  coord_flip()

```


# Random Forest 

## Model 

```{r, include=TRUE}

floor(sqrt(39))

(cores <- parallel::detectCores())

rf_def_mod <-
  rand_forest() %>% 
  set_engine("ranger",
             num.threads = cores, 
             importance = "permutation",  
             verbose = TRUE) %>% 
  set_mode("regression") %>% 
  set_args(mtry = NULL,
           trees = 1000,
           min_n = NULL) #doesn't run when mtry and min_n are tuned - have to be null to fit model two code chunks below 
           

translate(rf_def_mod)

```

## Workflow

```{r, include=TRUE}
rf_wflow <- workflow() %>%
  add_model(rf_def_mod) %>% 
  add_recipe(rec)
```

## Fit

```{r, include=TRUE}
tictoc::tic()
set.seed(3000)
rf_def_res <- fit_resamples(
  rf_wflow,
  data_cv,
  metrics = metrics_eval,
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x)
                              )
tictoc::tic()

head(rf_def_res)
```

## ## Best Estimates 

```{r, include=TRUE}
show_best(rf_def_res, "rmse")

show_best(rf_def_res, "rsq") 

show_best(rf_def_res, "huber_loss")
```

## Function Code Provided 

```{r, include=TRUE}
rf_tree_roots <- function(x){
  map_chr(1:1000, 
           ~ranger::treeInfo(x, tree = .)[1, "splitvarName"])
}

rf_roots <- function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(fit = map(.extracts,
                   ~.x$fit$fit$fit),
         oob_rmse = map_dbl(fit,
                         ~sqrt(.x$prediction.error)),
         roots = map(fit, 
                        ~rf_tree_roots(.))
         ) %>% 
  select(roots) %>% 
  unnest(cols = c(roots))
}

rf_roots(rf_def_res)

forest_roots <- rf_roots(rf_def_res)

```

## Plot

```{r, include=TRUE}
rf_root_count <- forest_roots %>% 
  count(roots)

ggplot(rf_root_count, aes(fct_reorder(roots, n), n)) +
  geom_col() +
  coord_flip()

```

7. The plots appear different because the bagged tree applies all of the predictors, and the most important are selected more often. Conversely, the random forest model uses only a subset of predictors to split the root, so there is a greater chance for the less important predictors to be chosen. 

## Apply Fit Function to Workflow and Full Training 

```{r, include=TRUE}
tictoc::tic()
fit_rf_workflow <- fit(rf_wflow, train)
tictoc::toc()

#1.696 seconds elapsed 


fit_rf_workflow

# OOB Prediction error (MSE): 9793.118

sqrt(9793.118)

# RMSE = 98.96

sqrt(fit_rf_workflow$fit$fit$fit$prediction.error)
```

OOB RMSE = 98.96

10 fold CV RMSE = 98.3 

There is less variance and higher bias in the random forest 10-fold CV model than the random forest OOB. The 10-fold CV likely influences less variance and more bias in the model. 

# Compare Performance 

```{r, include=TRUE}
# performance metrics of 4 models: 
show_best(tune_tree, metric = "rmse", n = 10) %>%
bind_rows(show_best(tune_tree, metric = "rsq", n = 10))%>%
bind_rows(show_best(tune_tree, metric = "huber_loss", n = 10))%>%
group_by(.metric) %>%
slice(1:3)

show_best(bag_tune, metric = "rmse", n = 10) %>%
bind_rows(show_best(bag_tune, metric = "rsq", n = 10))%>%
bind_rows(show_best(bag_tune, metric = "huber_loss", n = 10))%>%
group_by(.metric) %>%
slice(1:3)

show_best(rf_def_res, metric = "rmse", n = 10) %>%
bind_rows(show_best(rf_def_res, metric = "rsq", n = 10))%>%
bind_rows(show_best(rf_def_res, metric = "huber_loss", n = 10))%>%
group_by(.metric) %>%
slice(1:3)

show_best(fit_rf_workflow, metric = "rmse", n = 10) %>%
bind_rows(show_best(fit_rf_workflow, metric = "rsq", n = 10))%>%
bind_rows(show_best(fit_rf_workflow, metric = "huber_loss", n = 10))%>%
group_by(.metric) %>%
slice(1:3)

#library(stacks)

#wind_data_st <- 
 # stacks() %>%
  #add_candidates(tune_tree) %>%
  #add_candidates(bag_tune) %>%
  #add_candidates(rf_def_res) %>% 
  #add_candidates(fit_rf_workflow)

# couldn't download stack() package



```



